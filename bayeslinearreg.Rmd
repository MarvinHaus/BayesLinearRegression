---
title: "Bayesian Linear Regression mittels Metropolis-Hastings Algorithmus"
author: "Marvin Hauser, Sarem Seitz"
date: "11 August 2017"
output: html_document
---

```{r setup, include=FALSE}
library(MCMCpack)
library(truncnorm)
```
# Kurze Einführung
Das Thema unserer Arbeit ist die Programmierung eines Metropolis-Hastings Algorithmus zur simulatorischen Berechnung einer zweidimensionalen Normal-Inverse Gamma Posterior-Dichte. Da sich diese Dichte auch bei Bayesianischer Linearer Regression häufig wiederfindet, richten wir unser Programm direkt darauf aus, im Anschluss das genannte Regressionsverfahren in einer Funktion durchzuführen und mithilfe simulierter Daten das Verfahren zu überprüfen.

# Funktionen für den Metropolis-Hastings Algorithmus

## Log-Likelihood Funktionen
Berechnet die logarithmierte Likelihood für das Lineare Modell und summiert über alle Beobachtungen - Matrixschreibweise wurde gewählt um Schleifen zu vermeiden:

```{r loglikelihood}
logLikelihood = function(X,y,betas,sd)
{
  yHat = X%*%betas

  sumLikelihoods = sum(
                      dnorm(y, mean=yHat, sd = sd, log = TRUE)
                      )
  
  return(sumLikelihoods)
}
```

Berechnet logarithmierte 'Likelihood'-Werte der Prior Verteilungen und summiert diese über alle Prior-
verteilungen auf. Die Hyperparameter der Priorverteilungen können flexibel angepasst werden - ein einzelner
Skalar für die Parameter der Beta-Priors wird dann für alle Beta-Priors verwendendet; alternativ können auch
Prior-Parameter für jede Prior einzeln in Form eines Vektors eingegeben werden (siehe die eigentliche Funktion 'metropolisBayesRegression').

```{r loglikePrior}
logPriors = function(betas,sd, meanNormalPrior = 0, sdNormalPrior = 5, shapeInvGammaPrior = 1, scaleInvGamma = 1)
{
  lenBetas = length(betas)
  if (length(meanNormalPrior) == 1)
  {
    meanNormalPrior = rep(meanNormalPrior,lenBetas)
  }
  
  if (length(sdNormalPrior) == 1)
  {
    sdNormalPrior = rep(sdNormalPrior,lenBetas)
  }
  
  sumLogBetasPrior = sum(
    dnorm(betas, mean = meanNormalPrior,
          sd = sdNormalPrior, log = TRUE)
    )
  
  
  logSdPrior = log(dinvgamma(sd, shape=shapeInvGammaPrior, scale=scaleInvGamma))
  
  return(sumLogBetasPrior + logSdPrior)

}
```


## Posterior Density
Summiert alle Likelihood-Funktionen zur Berechnung der (Log) Posterior-Density auf

```{r posteriorLikelihood}
posterior <- function(X, y, betas, sd, meanNormalPrior=0, sdNormalPrior=5, shapeInvGammaPrior=1,scaleInvGammaPrior=1){
  
  return (
    logLikelihood(X, y, betas, sd) +
      logPriors(betas, sd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)
  )
}
```

## Proposal Densities (hier eigentlich Funktionen)

Um möglichst flexibel mit verschiedenen Proposal-Densities experimentieren zu können, können diese
in Form der unten programmierten Funktionen direkt als Argument an die eigentlich Regressions-Funktion
eingegeben werden.

### Proposal Function 1
Die Idee hinter proposalFunction1 ist die Simulierung eines klassichen RandomWalk-Prozesses mit Normalverteilung. Da der Varianz- (bzw. in R ja der Standardabweichungs-)parameter > 0 sein muss
und mit jeder Proposal-Standardabweichung <=0 eine Realisation der Markov-Chain sofort verworfen
werden würde (NaN-Werte in den Posterior-Densitites bekommen weiter unten den Dichtewert 0, damit der
MH-Algorithmus unproblematisch weiter läuft), wird als Proposal-Density für die Standardabweichung eine
trunkierte Normalverteilung mit 0 als Lower-Bound genutzt.
```{r proposal function 1}


proposalFunction1 <- function(betas, sd, betaProposalSd = 0.5, sdProposalSd = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = rnorm(lenBetas, mean = betas, sd = betaProposalSd)
  proposalSd = rtruncnorm(1,mean=sd, sd=sdProposalSd, a=0, b=Inf)
  
  return(list(proposalBetas, proposalSd))
  
}
```


### Proposal Function 2
proposalFunction2 nutzt eine gewöhnliche Normalverteilung wie häufig vorgeschlagen auch für die 
Standardabweichung. Entsprechend sollten für die Skalenparameter (hier 'sdProposalSd') auch entsprechend
kleinere Werte gewählt werden als bei der trunkierten Normalverteilung, um allzu häufige Sprünge in 
den Bereich <=0 zu vermeiden.
```{r proposal function 2}
proposalFunction2 <- function(betas, sd, betaProposalSd = 0.5, sdProposalSd = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = rnorm(lenBetas, mean = betas, sd = betaProposalSd)
  proposalSd = rnorm(1,mean=sd, sd=sdProposalSd)
  
  return(list(proposalBetas, proposalSd))
  
}
```


### Proposal Function 3

Die letzte Proposal Function arbeitet mit Gleichverteilungen. Um auch hier negative Proposals für die
Standardabweichung zu vermeiden, wird die untere Grenze des entsprechenden RNGs bei 0 trunkiert. Die
obere Grenze wird so festgelegt, dass in jedem Fall die aktuelle Standardabweichung den Erwartungswert
der Gleichverteilung bildet.
```{r proposal function 3}
proposalFunction3 <- function(betas, sd, betaProposalInterval = 0.5, sdProposalInterval = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = runif(lenBetas, min = betas-betaProposalInterval, max = betas + betaProposalInterval)
  
  unifLowerBound = max(sd-sdProposalInterval,0)
  unifUpperBound = sd + (sd-unifLowerBound)
  proposalSd = runif(1,min = unifLowerBound, max = unifUpperBound)
  
  return(list(proposalBetas, proposalSd))
  
}

```




## Bayesian Linear Regression (MH-Algorithmus)

Der eigentliche MH-Algorithmus. Es kann eine beliebige Anzahl an Features verwendet werden (in R allerdings ineffizient - auch da nicht parallelisierbar)

Erklärung der Parameter:

* **X**: Matrix der exogenen Variablen (wichtig ist, dass der Datentyp in R "matrix" und nicht "data.frame" ist, da sonst die Matrixoperationen fehlschlagen)
* **y**: Vektor/Matrix der endogenen Variable
* **startValuesBeta**: Startwerte für die Beta-Parameter im MH-Algorithmus
* **startValueSd**: Startwerte für Standardabweichung
* **nIterations**: Anzahl der Iterationen im MH-Algorithmus **nach** der Burn-In Phase
* **nBurnIn**: Anzahl der Burn-In Iterationen
* **meanNormalPrior**: Skalar **oder** Vektor der/des Erwartungswerte(s) der Beta-Prior(s) - wird lediglich ein Skalar übergeben, wird dieser für alle Beta-Priors verwendet
* **sdNormalPrior**: Wie *meanNormalPrior* für die Standardabweichung der Beta-Prior
* **shapeInvGamma**: Skalar für den Shape-Parameter der Inversen Gammaverteilung der Prior der Standardabweichung
* **scaleInvGamma**: Scale-Parameter der Inversen Gammaverteilung.
* **proposalFunction**: individuelle Proposal-Funktion (siehe entsprechenden Punkt weiter oben)
* **betaProposalScaleParameter**: Skalenparameter der ProposalFunktion der Betas - nicht zwangsläufig die Varianz/bzw. Standardabweichung (abhängig von der Proposal-Verteilung)
* **sdProposalScaleParameter**: Skalenparameter der ProposalFunktion der Standardabweichung

Da die acceptance-rate im MH-Algorithmus vor allem von den gewählten Skalenparametern abhängt können hier entsprechende Anpassungen vorgenommen werden.

```{r metropolis Linear Bayes Regression}
metropolisBayesRegression = function (X, y, startValuesBeta, startValueSd,
                          nIterations=10000, nBurnIn=500, meanNormalPrior=0, sdNormalPrior=5, shapeInvGammaPrior=1, scaleInvGammaPrior = 1, proposalFunction = proposalFunction1, betaProposalScaleParameter = 0.5, sdProposalScaleParameter = 0.5)
{
  #Sanity check
  if(ncol(X)!=length(startValuesBeta))
  {
    stop("nrow(X) does not match length(startValuesBeta)")
  }

  
  totalIterations = nIterations + nBurnIn
  lenBetas = ncol(X)
  
  markovChain = data.frame(matrix(NaN, nrow = totalIterations+1, ncol=lenBetas+1))
  colnames(markovChain) = c(paste("BETA",seq(0,lenBetas-1)),"SD")
  
  markovChain[1, 1:lenBetas] = startValuesBeta
  markovChain[1, lenBetas+1] = startValueSd
  
  for (iteration in 1:totalIterations)
  {
    currentBetas = as.numeric(markovChain[iteration, 1:lenBetas])
    currentSd = as.numeric(markovChain[iteration, lenBetas+1])
    
    proposalValues = proposalFunction(currentBetas, currentSd, betaProposalScaleParameter, sdProposalScaleParameter)
    
    #damit die Proposal-Function nur ein mal benötigt wird, gibt sie alle Proposals gleichzeitig als Liste aus
    proposalBetas = proposalValues[[1]]
    proposalSd = proposalValues[[2]]
    
    currentPosterior = posterior(X, y, currentBetas, currentSd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)
    proposalPosterior = posterior(X, y, proposalBetas, proposalSd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)

    #re-transformation des (Log-)Likelihood-Ratios um die Random-Acceptance durchzuführen
    likelihoodRatio = exp(proposalPosterior - currentPosterior)
    
    #gerade bei unzulässigen Proposal-Werten für die Varianz bzw. Standardabweichung ist der Wert des Likelihood-Ratios 'NaN'. Damit hier keine Probleme im if-statement weiter unten entstehen, ersetzen wir hier den NaN-Wert mit 0, was zu einer sofortigen Rejection der Proposal-Werte führt. Im großen und Ganzen möchten wir aber versuchen, unzulässige Werte für unsere Parameter direkt zu vereiden, um nicht unnötig viele Iterationen zu verschwenden.
    if (is.nan(likelihoodRatio))
      {
        likelihoodRatio = 0
      }
    
    
    comparisonValue = min(likelihoodRatio, 1)
    
    
    if(runif(1)<comparisonValue)
    {
      markovChain[iteration+1, 1:lenBetas] = proposalBetas
      markovChain[iteration+1, lenBetas+1] = proposalSd
    }
    else
    {
      markovChain[iteration+1,] = markovChain[iteration,]
    }
  
  }
  
  acceptanceRate = 1-mean(duplicated(markovChain[-(1:nBurnIn),]))

  #Wenn Beta ein einzelner Skalar ist, kann colMeans nicht ohne weitere Transformation angewandt werden
  if (lenBetas>1)
  {
    meanBetas = colMeans(markovChain[-(1:nBurnIn),1:lenBetas])
  }
  else
  {
    meanBetas = mean(markovChain[-(1:nBurnIn),1:lenBetas])
  }

  meanSd = mean(markovChain[-(1:nBurnIn), lenBetas+1])
  
  simulationResult = list(meanBetas, meanSd, markovChain[-(1:nBurnIn),],
                          markovChain, markovChain[(1:nBurnIn),], acceptanceRate) 
  
  names(simulationResult) = c("meanBetas", "meanSd", "chainNoBurnIn", "wholeChain", "chainBurnInOnly", "acceptanceRate")
  
  return(simulationResult)
}

```


#Experimenteller Teil

Im experimentellen Teil werden wir den oben vorgestellten Metropolis-Algorithmus zuerst auf einen künstlich erzeugten Datensatz anwenden, bei dem alle Parameter a priori bekannt sind. Dies ermöglicht es uns vor allem, die Ergebnisse unseres Programms auf Korrektheit zu überprüfen.


## Generierung zufälliger Testdaten

Da die Funktion 'metropolisBayesRegression' auf Regression in Matrix-Notation ausgelegt ist, werden die Testdaten entsprechend generiert.

```{r generateTestData}
set.seed(5234)

beta0 = -2.234
beta1 = 1.345

sampleSize = 100
standardDeviation = 5

X = cbind(rep(1,sampleSize), runif(sampleSize, -10,10))
y = X%*%c(beta0,beta1) + rnorm(sampleSize, 0, standardDeviation)

```

Für ggplot werden X und y zu einem data.frame Objekt zusammengefasst:

```{r prepareForPlot}
ggData = as.data.frame(cbind(y,X[,2]))
colnames(ggData) = c("endog", "exog")

library(ggplot2)
```

```{r plot, echo=FALSE, align="center"}

plotTestData <- ggplot(ggData, aes(x = exog, y = endog))+geom_point()+geom_abline(intercept = beta0, slope = beta1,colour="red")
plotTestData
```

## Bestimmung der Prior-Parameter
Um möglichst sinnvolle Werte für unsere Prior-Verteilungen festlegen zu können, teilen wir den künstlich erzeugten Datensatz zufällig in zwei Teile à 1/10 und 9/10 auf. Den kleineren Teil nutzen wir, um aus einer einfachen linearen Regression 'a-priori' Informationen über die Regressionsparameter zu gewinnen. Würden wir uns streng an das Bayes-Paradigma halten, müssten wir hier mit uninformativen Priors arbeiten - da der Fokus unserer Arbeit aber auf Normal-/Inverse Gamma Priors liegt, arbeiten wir an dieser Stelle noch mit klassischer Regression. 

```{r aPrioriRegression}
set.seed(3456)

priorDataPoints = sample(1:sampleSize, round(sampleSize/10))

priorX = X[priorDataPoints,]
priorY = y[priorDataPoints,]

#hier werden die Datenpunkte für die eigentliche Bayes-Regression bestimmt
bayesX = X[-priorDataPoints,]
bayesY = y[-priorDataPoints]

priorRegression = lm(priorY~0+priorX)

print(summary(priorRegression))

```

Wir nehmen nun an, dass die oben errechneten Werte aus einer vorhergehenden Studie stammen und uns lediglich die Regressionsergebnisse vorliegen, nicht aber die einzelnen Daten (da wir ansonsten einfach die a priori Daten mit unseren vereinen und die Regression anhand der vereinigten Daten durchführen könnten, was informative Prior-Verteilungen wieder hinfällig machen würde)
```{r priorSetting}
betaPriorMeans = priorRegression$coefficients
betaPriorSDs = as.numeric(summary(priorRegression)[[4]][,2])

sdPriorExpectation = summary(priorRegression)[[6]]

```

Die Prior Parameter der Betas können wir direkt aus den Ergebnissen der Regression übernehmen. Um die Inverse Gamma Prior für die Standardabweichung vollständig spezifizieren zu können, benötigen wir Shape- und Scaleparameter Alpha und Beta. Um diese zu bestimmen, betrachten wir Erwartungswert und Varianz der Inversen Gammaverteilung: 
$$\mathbb{E}_{InvGamma}(\alpha, \beta)=\tfrac{\beta}{\alpha-1} \quad \forall \alpha>1$$
sowie
$$\mathbb{Var}_{InvGamma}(\alpha, \beta)=\tfrac{\beta^2}{(\alpha-1)^2(\alpha-2)} \quad \forall \alpha>2$$

Als Erwartungswert für die Prior Inverse Gammmaverteilung wählen wir aufgrund des relativ hohen Gütemaßes in der "Prior-Regression" auch die geschätzte Standardabweichung ebendieser Regression. Da wir außerdem aufgrund des hohen Gütemaßes davon ausgehen können, dass die geschätzte Standardabweichung relativ nah bei der tatsächlichen Standardabweichung liegt (was sich bei Blick auf die zuvor definierte, wahre Standardabweichung auch bestätig), sollte sich dies auch in einer relativ geringen Varianz der Prior Inverse Gamma wiederspiegeln. Damit diese Varianz existiert, muss der Shape-Parameter Alpha wiederum größer als **2** sein. Damit außerdem die Varianz nicht zu groß wird, sollten Shape- und Skalenparameter nicht zu weit auseinander liegen, bzw. aufgrund der Potenzen im besten Fall beide relativ klein sein - hier legen wir diese auf **3.5** fest, um eine gewisse Sicherheit bezüglich der tatsächlichen Standardabweichung auszudrücken. Daraus ergeben sich dementsprechend folgende Bedingungen für die Parameter der Inversen Gamma Prior:
$$\tfrac{\beta}{\alpha-1}\overset{!}{=}\hat{SD}_{'PriorRegression'}$$
$$\tfrac{\beta^2}{(\alpha-1)^2(\alpha-2)}\overset{!}{=}3.5^2$$
sowie die Nebenbedingung:
$$\alpha\overset{!}{>}2$$
(zusätzlich zu der allgemeinen Voraussetzung, dass Shape- und Scaleparameter einer Inversen Gammaverteilung größer als 0 sein müssen, was aber unter den o.g. Bedingungen in jedem Fall gegeben ist)

Damit ergeben sich die beiden Parameter der Inverse Gammaverteilung zu:
$$\alpha=(\tfrac{\hat{SD}_{'PriorRegression'}}{3.5})^2+2 \quad (>2)$$
$$\beta=\hat{SD}_{'PriorRegression'}(\alpha-1)$$
```{r invGammaParameters}
alphaPrior = (sdPriorExpectation/3.5)^2+2
betaPrior = sdPriorExpectation*(alphaPrior-1)


#kurzer Sanity-Check:
print("SD nach PriorRegression")
print(sdPriorExpectation)
print("Erwartungswert Inverse Gamma")
print(betaPrior/(alphaPrior-1))
print("Standardabweichung Inverse Gamma")
print(sqrt(betaPrior^2/((alphaPrior-1)^2*(alphaPrior-2))))

```

Nachdem wir die Parameter für die Prior-Verteilungen festgelegt haben, können wir jetzt die eigentliche Regression durchführen. Da es im Grunde unendlich viele Möglichkeiten gibt, die Ausgangsverteilungen und Hyperparameter für Prior- und Proposalfunktionen zu bestimmen, beschränken wir uns hier auf die oben vorgestellten Proposal-Funktionen, sowie die gerade definierten Parameter für die Prior-Verteilungen.
Als Startwerte werden wir jeweils drei verschiedene Startmöglichkeiten pro Proposal-Funktionen verwenden - die Skalenparameter der Proposal-Densities haben wir durch Trial&Error so angepasst, dass die Acceptance-Rate in etwa den von Gelman für optimal erklärten Wert von 0.234 erreicht.