---
title: "Bayesian Linear Regression mittels Metropolis-Hastings Algorithmus"
author: "Marvin Hauser, Sarem Seitz"
date: "11 August 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(MCMCpack)
library(truncnorm)
```
# Kurze Einführung
Das Thema unserer Arbeit ist die Programmierung eines Metropolis-Hastings Algorithmus zur simulatorischen Berechnung einer zweidimensionalen Normal-Inverse Gamma Posterior-Dichte. Da sich diese Dichte auch bei Bayesianischer Linearer Regression häufig wiederfindet, richten wir unser Programm direkt darauf aus, im Anschluss das genannte Regressionsverfahren in einer Funktion durchzuführen und mithilfe simulierter Daten das Verfahren zu überprüfen.

# Funktionen für den Metropolis-Hastings Algorithmus

## Log-Likelihood Funktionen
Berechnet die logarithmierte Likelihood für das Lineare Modell und summiert über alle Beobachtungen - Matrixschreibweise wurde gewählt um Schleifen zu vermeiden:

```{r loglikelihood}
logLikelihood = function(X,y,betas,sd)
{
  yHat = X%*%betas

  sumLikelihoods = sum(
                      dnorm(y, mean=yHat, sd = sd, log = TRUE)
                      )
  
  return(sumLikelihoods)
}
```

Berechnet logarithmierte 'Likelihood'-Werte der Prior Verteilungen und summiert diese über alle Prior-
verteilungen auf. Die Hyperparameter der Priorverteilungen können flexibel angepasst werden - ein einzelner
Skalar für die Parameter der Beta-Priors wird dann für alle Beta-Priors verwendendet; alternativ können auch
Prior-Parameter für jede Prior einzeln in Form eines Vektors eingegeben werden (siehe die eigentliche Funktion 'metropolisBayesRegression').

```{r loglikePrior}
logPriors = function(betas,sd, meanNormalPrior = 0, sdNormalPrior = 5, shapeInvGammaPrior = 1, scaleInvGamma = 1)
{
  lenBetas = length(betas)
  if (length(meanNormalPrior) == 1)
  {
    meanNormalPrior = rep(meanNormalPrior,lenBetas)
  }
  
  if (length(sdNormalPrior) == 1)
  {
    sdNormalPrior = rep(sdNormalPrior,lenBetas)
  }
  
  sumLogBetasPrior = sum(
    dnorm(betas, mean = meanNormalPrior,
          sd = sdNormalPrior, log = TRUE)
    )
  
  
  logSdPrior = log(dinvgamma(sd, shape=shapeInvGammaPrior, scale=scaleInvGamma))
  
  return(sumLogBetasPrior + logSdPrior)

}
```


## Posterior Density
Summiert alle Likelihood-Funktionen zur Berechnung der (Log) Posterior-Density auf

```{r posteriorLikelihood}
posterior <- function(X, y, betas, sd, meanNormalPrior=0, sdNormalPrior=5, shapeInvGammaPrior=1,scaleInvGammaPrior=1){
  
  return (
    logLikelihood(X, y, betas, sd) +
      logPriors(betas, sd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)
  )
}
```

## Proposal Densities (hier eigentlich Funktionen)

Um möglichst flexibel mit verschiedenen Proposal-Densities experimentieren zu können, können diese
in Form der unten programmierten Funktionen direkt als Argument an die eigentlich Regressions-Funktion
eingegeben werden.

### Proposal Function 1
Die Idee hinter proposalFunction1 ist die Simulierung eines klassichen RandomWalk-Prozesses mit Normalverteilung. Da der Varianz- (bzw. in R ja der Standardabweichungs-)parameter > 0 sein muss
und mit jeder Proposal-Standardabweichung <=0 eine Realisation der Markov-Chain sofort verworfen
werden würde (NaN-Werte in den Posterior-Densitites bekommen weiter unten den Dichtewert 0, damit der
MH-Algorithmus unproblematisch weiter läuft), wird als Proposal-Density für die Standardabweichung eine
trunkierte Normalverteilung mit 0 als Lower-Bound genutzt.
```{r proposal function 1}


proposalFunction1 <- function(betas, sd, betaProposalSd = 0.5, sdProposalSd = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = rnorm(lenBetas, mean = betas, sd = betaProposalSd)
  proposalSd = rtruncnorm(1,mean=sd, sd=sdProposalSd, a=0, b=Inf)
  
  return(list(proposalBetas, proposalSd))
  
}
```


### Proposal Function 2
proposalFunction2 nutzt eine t-Verteilung für die Betas und eine log-Normalverteilung für die Proposal der Standardabweichung. Damit die t-Verteilung eine endliche Varianz besitzt, muss die Anzahl der Freiheitsgrade >2 sein - wir haben uns für df = 3 entschieden, um einen deutlichen Unterschied zur Normalverteilung zu gewährleisten.
```{r proposal function 2}
proposalFunction2 <- function(betas, sd, betaProposalSd = 0.5, sdProposalSd = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = rt(lenBetas, df = 3)*betaProposalSd
  proposalSd = rlnorm(1,meanlog=sd, sd=sdProposalSd)
  
  return(list(proposalBetas, proposalSd))
  
}
```


### Proposal Function 3

Die letzte Proposal Function arbeitet mit Gleichverteilungen. Um auch hier negative Proposals für die
Standardabweichung zu vermeiden, wird die untere Grenze des entsprechenden RNGs bei 0 trunkiert. Die
obere Grenze wird so festgelegt, dass in jedem Fall die aktuelle Standardabweichung den Erwartungswert
der Gleichverteilung bildet.
```{r proposal function 3}
proposalFunction3 <- function(betas, sd, betaProposalInterval = 0.5, sdProposalInterval = 0.5){
  
  lenBetas = length(betas)
  
  proposalBetas = runif(lenBetas, min = betas-betaProposalInterval, max = betas + betaProposalInterval)
  
  unifLowerBound = max(sd-sdProposalInterval,0)
  unifUpperBound = sd + (sd-unifLowerBound)
  proposalSd = runif(1,min = unifLowerBound, max = unifUpperBound)
  
  return(list(proposalBetas, proposalSd))
  
}

```




## Bayesian Linear Regression (MH-Algorithmus)

Der eigentliche MH-Algorithmus. Es kann eine beliebige Anzahl an Features verwendet werden (in R allerdings ineffizient - auch da nicht parallelisierbar)

Erklärung der Parameter:

* **X**: Matrix der exogenen Variablen (wichtig ist, dass der Datentyp in R "matrix" und nicht "data.frame" ist, da sonst die Matrixoperationen fehlschlagen)
* **y**: Vektor/Matrix der endogenen Variable
* **startValuesBeta**: Startwerte für die Beta-Parameter im MH-Algorithmus
* **startValueSd**: Startwerte für Standardabweichung
* **nIterations**: Anzahl der Iterationen im MH-Algorithmus **nach** der Burn-In Phase
* **nBurnIn**: Anzahl der Burn-In Iterationen
* **meanNormalPrior**: Skalar **oder** Vektor der/des Erwartungswerte(s) der Beta-Prior(s) - wird lediglich ein Skalar übergeben, wird dieser für alle Beta-Priors verwendet
* **sdNormalPrior**: Wie *meanNormalPrior* für die Standardabweichung der Beta-Prior
* **shapeInvGamma**: Skalar für den Shape-Parameter der Inversen Gammaverteilung der Prior der Standardabweichung
* **scaleInvGamma**: Scale-Parameter der Inversen Gammaverteilung.
* **proposalFunction**: individuelle Proposal-Funktion (siehe entsprechenden Punkt weiter oben)
* **betaProposalScaleParameter**: Skalenparameter der ProposalFunktion der Betas - nicht zwangsläufig die Varianz/bzw. Standardabweichung (abhängig von der Proposal-Verteilung)
* **sdProposalScaleParameter**: Skalenparameter der ProposalFunktion der Standardabweichung

Da die acceptance-rate im MH-Algorithmus vor allem von den gewählten Skalenparametern abhängt können hier entsprechende Anpassungen vorgenommen werden.

```{r metropolis Linear Bayes Regression}
metropolisBayesRegression = function (X, y, startValuesBeta, startValueSd,
                          nIterations=10000, nBurnIn=500, meanNormalPrior=0, sdNormalPrior=5, shapeInvGammaPrior=1, scaleInvGammaPrior = 1, proposalFunction = proposalFunction1, betaProposalScaleParameter = 0.5, sdProposalScaleParameter = 0.5)
{
  #Sanity check
  if(ncol(X)!=length(startValuesBeta))
  {
    stop("nrow(X) does not match length(startValuesBeta)")
  }

  
  totalIterations = nIterations + nBurnIn
  lenBetas = ncol(X)
  
  markovChain = data.frame(matrix(NaN, nrow = totalIterations+1, ncol=lenBetas+2))
  colnames(markovChain) = c(paste("beta",seq(0,lenBetas-1),sep=""),"SD","iter")
  
  
  markovChain[1, 1:lenBetas] = startValuesBeta
  markovChain[1, lenBetas+1] = startValueSd
  
  for (iteration in 1:totalIterations)
  {
    currentBetas = as.numeric(markovChain[iteration, 1:lenBetas])
    currentSd = as.numeric(markovChain[iteration, lenBetas+1])
    
    proposalValues = proposalFunction(currentBetas, currentSd, betaProposalScaleParameter, sdProposalScaleParameter)
    
    #damit die Proposal-Function nur ein mal benötigt wird, gibt sie alle Proposals gleichzeitig als Liste aus
    proposalBetas = proposalValues[[1]]
    proposalSd = proposalValues[[2]]
    
    currentPosterior = posterior(X, y, currentBetas, currentSd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)
    proposalPosterior = posterior(X, y, proposalBetas, proposalSd, meanNormalPrior, sdNormalPrior, shapeInvGammaPrior, scaleInvGammaPrior)

    #re-transformation des (Log-)Likelihood-Ratios um die Random-Acceptance durchzuführen
    likelihoodRatio = exp(proposalPosterior - currentPosterior)
    
    #gerade bei unzulässigen Proposal-Werten für die Varianz bzw. Standardabweichung ist der Wert des Likelihood-Ratios 'NaN'. Damit hier keine Probleme im if-statement weiter unten entstehen, ersetzen wir hier den NaN-Wert mit 0, was zu einer sofortigen Rejection der Proposal-Werte führt. Im großen und Ganzen möchten wir aber versuchen, unzulässige Werte für unsere Parameter direkt zu vereiden, um nicht unnötig viele Iterationen zu verschwenden.
    if (is.nan(likelihoodRatio))
      {
        likelihoodRatio = 0
      }
    
    
    comparisonValue = min(likelihoodRatio, 1)
    
    
    if(runif(1)<comparisonValue)
    {
      markovChain[iteration+1, 1:lenBetas] = proposalBetas
      markovChain[iteration+1, lenBetas+1] = proposalSd
    }
    else
    {
      markovChain[iteration+1,] = markovChain[iteration,]
    }
  
  }
  
  acceptanceRate = 1-mean(duplicated(markovChain[-(1:nBurnIn),]))

  #Wenn Beta ein einzelner Skalar ist, kann colMeans nicht ohne weitere Transformation angewandt werden
  if (lenBetas>1)
  {
    meanBetas = colMeans(markovChain[-(1:nBurnIn),1:lenBetas])
  }
  else
  {
    meanBetas = mean(markovChain[-(1:nBurnIn),1:lenBetas])
  }

  meanSd = mean(markovChain[-(1:nBurnIn), lenBetas+1])
  
  #eine extra-Spalte für die Nummer der Iteration erleichtert das Plotten mit ggplot
  markovChain$iter = seq(0,totalIterations)

  simulationResult = list(meanBetas, meanSd, markovChain[-(1:nBurnIn),],
                          markovChain, markovChain[(1:nBurnIn),], acceptanceRate) 
  
  names(simulationResult) = c("meanBetas", "meanSd", "chainNoBurnIn", "wholeChain", "chainBurnInOnly", "acceptanceRate")
  
  return(simulationResult)
}

```


#Experimenteller Teil

Im experimentellen Teil werden wir den oben vorgestellten Metropolis-Algorithmus zuerst auf einen künstlich erzeugten Datensatz anwenden, bei dem alle Parameter a priori bekannt sind. Dies ermöglicht es uns vor allem, die Ergebnisse unseres Programms auf Korrektheit zu überprüfen.


## Generierung zufälliger Testdaten

Da die Funktion 'metropolisBayesRegression' auf Regression in Matrix-Notation ausgelegt ist, werden die Testdaten entsprechend generiert.

```{r generateTestData}
set.seed(5234)

beta0 = -2.234
beta1 = 1.345

sampleSize = 100
standardDeviation = 5

X = cbind(rep(1,sampleSize), runif(sampleSize, -10,10))
y = X%*%c(beta0,beta1) + rnorm(sampleSize, 0, standardDeviation)

```

Für ggplot werden X und y zu einem data.frame Objekt zusammengefasst:

```{r prepareForPlot}
ggData = as.data.frame(cbind(y,X[,2]))
colnames(ggData) = c("endog", "exog")

library(ggplot2)
```

```{r plot, echo=FALSE, align="center"}

plotTestData <- ggplot(ggData, aes(x = exog, y = endog))+geom_point()+geom_abline(intercept = beta0, slope = beta1,colour="red")
plotTestData
```

## Bestimmung der Prior-Parameter
Um möglichst sinnvolle Werte für unsere Prior-Verteilungen festlegen zu können, teilen wir den künstlich erzeugten Datensatz zufällig in zwei Teile à 1/10 und 9/10 auf. Den kleineren Teil nutzen wir, um aus einer einfachen linearen Regression 'a-priori' Informationen über die Regressionsparameter zu gewinnen. Würden wir uns streng an das Bayes-Paradigma halten, müssten wir hier mit uninformativen Priors arbeiten - da der Fokus unserer Arbeit aber auf Normal-/Inverse Gamma Priors liegt, arbeiten wir an dieser Stelle noch mit klassischer Regression. 

```{r aPrioriRegression}
set.seed(3456)

priorDataPoints = sample(1:sampleSize, round(sampleSize/10))

priorX = X[priorDataPoints,]
priorY = y[priorDataPoints,]

#hier werden die Datenpunkte für die eigentliche Bayes-Regression bestimmt
bayesX = X[-priorDataPoints,]
bayesY = y[-priorDataPoints]

priorRegression = lm(priorY~0+priorX)

print(summary(priorRegression))

```

Wir nehmen nun an, dass die oben errechneten Werte aus einer vorhergehenden Studie stammen und uns lediglich die Regressionsergebnisse vorliegen, nicht aber die einzelnen Daten (da wir ansonsten einfach die a priori Daten mit unseren vereinen und die Regression anhand der vereinigten Daten durchführen könnten, was informative Prior-Verteilungen wieder hinfällig machen würde)
```{r priorSetting}
betaPriorMeans = priorRegression$coefficients
betaPriorSDs = as.numeric(summary(priorRegression)[[4]][,2])

sdPriorExpectation = summary(priorRegression)[[6]]

```

Die Prior Parameter der Betas können wir direkt aus den Ergebnissen der Regression übernehmen. Um die Inverse Gamma Prior für die Standardabweichung vollständig spezifizieren zu können, benötigen wir Shape- und Scaleparameter Alpha und Beta. Um diese zu bestimmen, betrachten wir Erwartungswert und Varianz der Inversen Gammaverteilung: 
$$\mathbb{E}_{InvGamma}(\alpha, \beta)=\tfrac{\beta}{\alpha-1} \quad \forall \alpha>1$$
sowie
$$\mathbb{Var}_{InvGamma}(\alpha, \beta)=\tfrac{\beta^2}{(\alpha-1)^2(\alpha-2)} \quad \forall \alpha>2$$

Als Erwartungswert für die Prior Inverse Gammmaverteilung wählen wir aufgrund des relativ hohen Gütemaßes in der "Prior-Regression" auch die geschätzte Standardabweichung ebendieser Regression. Da wir außerdem aufgrund des hohen Gütemaßes davon ausgehen können, dass die geschätzte Standardabweichung relativ nah bei der tatsächlichen Standardabweichung liegt (was sich bei Blick auf die zuvor definierte, wahre Standardabweichung auch bestätig), sollte sich dies auch in einer relativ geringen Varianz der Prior Inverse Gamma wiederspiegeln. Damit diese Varianz existiert, muss der Shape-Parameter Alpha wiederum größer als **2** sein. Damit außerdem die Varianz nicht zu groß wird, sollten Shape- und Skalenparameter nicht zu weit auseinander liegen, bzw. aufgrund der Potenzen im besten Fall beide relativ klein sein - hier legen wir diese auf **3.5** fest, um eine gewisse Sicherheit bezüglich der tatsächlichen Standardabweichung auszudrücken. Daraus ergeben sich dementsprechend folgende Bedingungen für die Parameter der Inversen Gamma Prior:
$$\tfrac{\beta}{\alpha-1}\overset{!}{=}\hat{SD}_{'PriorRegression'}$$
$$\tfrac{\beta^2}{(\alpha-1)^2(\alpha-2)}\overset{!}{=}3.5^2$$
sowie die Nebenbedingung:
$$\alpha\overset{!}{>}2$$
(zusätzlich zu der allgemeinen Voraussetzung, dass Shape- und Scaleparameter einer Inversen Gammaverteilung größer als 0 sein müssen, was aber unter den o.g. Bedingungen in jedem Fall gegeben ist)

Damit ergeben sich die beiden Parameter der Inverse Gammaverteilung zu:
$$\alpha=(\tfrac{\hat{SD}_{'PriorRegression'}}{3.5})^2+2 \quad (>2)$$
$$\beta=\hat{SD}_{'PriorRegression'}(\alpha-1)$$
```{r invGammaParameters}
alphaPrior = (sdPriorExpectation/3.5)^2+2
betaPrior = sdPriorExpectation*(alphaPrior-1)


#kurzer Sanity-Check:
print(sdPriorExpectation)
print(betaPrior/(alphaPrior-1))
print(sqrt(betaPrior^2/((alphaPrior-1)^2*(alphaPrior-2))))

```

## Bestimmung der Startwerte
Nachdem wir die Parameter für die Prior-Verteilungen festgelegt haben, können wir jetzt die eigentliche Regression durchführen. Da es im Grunde unendlich viele Möglichkeiten gibt, die Ausgangsverteilungen und Hyperparameter für Prior- und Proposalfunktionen zu bestimmen, beschränken wir uns hier auf die oben vorgestellten Proposal-Funktionen, sowie die gerade definierten Parameter für die Prior-Verteilungen.
Als Startwerte werden wir jeweils drei verschiedene Startmöglichkeiten pro Proposal-Funktionen verwenden, eine davon mit Startwerten nahe der wahren Werte, eine mit Startwerten für die Betas, die weit entfernt von ihren waren Werten liegen, sowie Startwerte, bei denen alle Werte weit entfernt von ihren wahren Werten liegen. Die Skalenparameter der Proposal-Densities haben wir durch Trial&Error so angepasst, dass die Acceptance-Rate von Proposal Function 1 bei Startwerten **nahe der wahren Parameter** in etwa den von Gelman für optimal erklärten Wert von 0.234 erreicht. Für alle anderen Proposal Funktionen werden dieselben  Werte für die Skalenparameter verwendet, um einen Vergleich zwischen den Proposal Densities zu erleichtern.



```{r startValuesPF1}
startValBeta1 = c(-1.5, 0.5)
startValSd1 = 4

startValBeta2 = c(10,-7)
startValSd2 = 4

startValBeta3 = c(10,-7)
startValSd3 = 15
```


## Durchführung der Regression

### Proposal Function 1
```{r seed1}
set.seed(876)
```

```{r regressionProposal1}
bayRegProp1start1 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta1, startValueSd = startValSd1, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction1, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp1start1$acceptanceRate)


bayRegProp1start2 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta2, startValueSd = startValSd2, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction1, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp1start2$acceptanceRate)


bayRegProp1start3 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta3, startValueSd = startValSd3, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction1, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp1start3$acceptanceRate)
```

### Proposal Funktion 2
```{r seed2}
set.seed(876)
```

```{r regressionProposal2}
bayRegProp2start1 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta1, startValueSd = startValSd1, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction2, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp2start1$acceptanceRate)


bayRegProp2start2 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta2, startValueSd = startValSd2, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction2, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp2start2$acceptanceRate)


bayRegProp2start3 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta3, startValueSd = startValSd3, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction2, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp2start3$acceptanceRate)
```


### Proposal Funktion 3
```{r seed3}
set.seed(876)
```

```{r regressionProposal3}
bayRegProp3start1 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta1, startValueSd = startValSd1, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction3, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp3start1$acceptanceRate)


bayRegProp3start2 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta2, startValueSd = startValSd2, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction3, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp3start2$acceptanceRate)


bayRegProp3start3 = metropolisBayesRegression(bayesX, bayesY, startValuesBeta = startValBeta3, startValueSd = startValSd3, nIterations = 10000, nBurnIn = 5000, meanNormalPrior = betaPriorMeans, sdNormalPrior = betaPriorSDs, shapeInvGammaPrior = alphaPrior, scaleInvGammaPrior = betaPrior, proposalFunction = proposalFunction3, betaProposalScaleParameter = 0.35, sdProposalScaleParameter = 0.35)

print(bayRegProp3start3$acceptanceRate)
```


Nach Durchführung der Regression mit Hilfe des Metropolis-Algorithmus sollen jetzt die Ergebnisse analysiert werden.

### Vergleich der unterschiedlichen Proposal-Funktionen

Für Proposal-Function 2 liegt die Acceptance-Rate deutlich unter der von Proposal Function 1 - bzw. es fand keine Acceptance über die gesamt Markov-Kette erkennbar. Dies ist damit begründbar, dass die Varianzen der entsprechenden Vorschlagsverteilungen ([skalierte] Student-t und log-Normal) bei gleichen Skalenparametern höhere Wahrscheinlichkeiten für extremere Werte haben. Dies führt dementsprechend zu höherer Wahrscheinlichkeit, bei bereits "guten" Parameterwerten in der Markov-Chain, den Bereich der Parameterwerte mit hohen Likelihoods sprunghaft wieder zu verlassen und damit eine Rejetion der Proposal-Werte zu 
Um also eine angemessene Acceptance-Rate zu erzielen, müssten wir die Skalenparameter deutlich reduzieren, was wir hier aber auslassen möchten.  
Im Gegensatz dazu ist die Acceptance-Rate bei Gleichverteilungen mit gleichem Skalenparameter (**hier allerdings nicht mit der Standardabweichung zu verwechseln**) deutlich höher als bei (trunkierter) Normalverteilung. Da die Wertebereiche bei den Gleichverteilungen abhängig von den Skalenparametern sind und diese hier relativ klein gehalten sind, sind Sprünge aus dem Parameterbereich mit hohen Likelihood-Werten deutlich unwahrscheinlicher als bei Normalverteilungen, deren Träger sich ja über alle reellen, bzw. alle positiven reellen Zahlen erstreckt.  

### Analyse der Markov Kette
Zum Abschluss widmen wir uns der eigentlichen Analyse der Likelihood, vor allem hinsichtlich Konvergenz und Parameterschätzung. Da die Analysen für alle Proposal-Funktionen ähnlich ablaufen würde (mit Ausnahme von Proposal-Funktion 2, wo wir ja in diesem Experiment kein verwertbares Ergebnis erzielt haben), beschränken wir diese Analyse auf Proposal-Funktion 1 mit jeweils drei Startparameter-Tupeln (beta0, beta1, Standardabweichung)  
   
Für die gesamte Markov-Kette, also inklusive BurnIn-Phase, ergibt sich für die drei Start-Werte folgendes Bild:

```{r multiplot, echo=FALSE}
#Funktion für multiplot - übernommen von http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  if (is.null(layout)) {

    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

## Analyse des Konvergenzverhaltens

### Startwerte nahe der wahren Werte
```{r mc1Convergence, echo = FALSE}
mc1Convp1 = ggplot(bayRegProp1start1$wholeChain, aes(x=iter, y=beta0)) + geom_line(color = 1
) + xlab("Iteration") + ylab("beta0") + ggtitle("Konvergenz beta0") + geom_hline(yintercept = beta0, color=4)

mc1Convp2 = ggplot(bayRegProp1start1$wholeChain, aes(x=iter, y=beta1)) + geom_line(color=2) + xlab("Iteration") + ylab("beta1") + ggtitle("Konvergenz beta1") + geom_hline(yintercept = beta1, color=4)

mc1Convp3 = ggplot(bayRegProp1start1$wholeChain, aes(x=iter, y=SD)) + geom_line(color=3) + xlab("Iteration") + ylab("SD") + ggtitle("Konvergenz SD") + geom_hline(yintercept = standardDeviation, color=4)

multiplot(mc1Convp1, mc1Convp2, mc1Convp3)
```

Für beta0 wird die Konvergenz sehr früh erreicht. Auch für die anderen beiden Parameter ist eine schnelle Konvergenz festzustellen - einzig sehr kurz nach Beginn der BurnIn-Phase sind starke Ausschläge zu erkennen. Ansonsten haben - mit Ausnahme der Standardabweichung um Iteration 9000 - die Markov-Ketten einen erkennbaren Konvergenzverlauf.

### Beta-Startwerte weiter von den wahren Parametern entfernt

```{r mc2Convergence, echo = FALSE}
mc2Convp1 = ggplot(bayRegProp1start2$wholeChain, aes(x=iter, y=beta0)) + geom_line(color = 1
) + xlab("Iteration") + ylab("beta0") + ggtitle("Konvergenz beta0") + geom_hline(yintercept = beta0, color=4)

mc2Convp2 = ggplot(bayRegProp1start2$wholeChain, aes(x=iter, y=beta1)) + geom_line(color=2) + xlab("Iteration") + ylab("beta1") + ggtitle("Konvergenz beta1")+ geom_hline(yintercept = beta1, color=4)

mc2Convp3 = ggplot(bayRegProp1start2$wholeChain, aes(x=iter, y=SD)) + geom_line(color=3) + xlab("Iteration") + ylab("SD") + ggtitle("Konvergenz SD") + geom_hline(yintercept = standardDeviation, color=4)

multiplot(mc2Convp1, mc2Convp2, mc2Convp3)
```

Wie zu erwarten benötigt die Markov-Kette zu Beginn der Burn-In Phase erkennbar länger, um Konvergenz zu erreichen. Auffällig ist, dass auch die Standardabweichung trotz Startwert nahe des wahren Wertes in etwa so lange wie die anderen Parameter benötigt, bis sich erkennbare Konvergenz einstellt. Eine mögliche Erklärung hierfür könnte sein, dass "unpassendere" Werte der Standardabweichung für bessere Posterior Likelihood-Werte bei ebenfalls weiter entfernten Beta-Parametern sorgt.

### Alle Startwerte weiter vom wahren Wert entfernt

```{r mc3Convergence, echo = FALSE}
mc3Convp1 = ggplot(bayRegProp1start3$wholeChain, aes(x=iter, y=beta0)) + geom_line(color = 1
) + xlab("Iteration") + ylab("beta0") + ggtitle("Konvergenz beta0") + geom_hline(yintercept = beta0, color=4)

mc3Convp2 = ggplot(bayRegProp1start3$wholeChain, aes(x=iter, y=beta1)) + geom_line(color=2) + xlab("Iteration") + ylab("beta1") + ggtitle("Konvergenz beta1") + geom_hline(yintercept = beta1, color=4)

mc3Convp3 = ggplot(bayRegProp1start3$wholeChain, aes(x=iter, y=SD)) + geom_line(color=3) + xlab("Iteration") + ylab("SD") + ggtitle("Konvergenz SD") + geom_hline(yintercept = standardDeviation, color=4)

multiplot(mc3Convp1, mc3Convp2, mc3Convp3)
```

Auch hier erreicht die Markov-Kette die Konvergenz trotz anfangs relativ weit von den wahren Parameterwerten entfernten Startwerten. Da mit allen drei Startwerten ein ähnlicher Konvergenzzustand hinsichtlich der wahren Parameterwerte erreicht wird, beschränken wir die weitere Analyse auf den Fall von Startwerten nahe der wahren Parameterwerte

## ACF der Markov-Ketten nach BurnIn-Phase
Nun wollen wir nachvollziehen, wie stark die Autokorrleation zwischen den Iterationen der Markov-Kette 
ist. Wir beschränken uns aus ACF-Plots.

```{r ACFs, echo = FALSE}
ac1 = as.data.frame(cbind(acf(bayRegProp1start1$chainNoBurnIn[,1], plot = FALSE)$lag,
                          acf(bayRegProp1start1$chainNoBurnIn[,1], plot = FALSE)$acf))
colnames(ac1) = c("lag", "acf")
                    
ac2 = as.data.frame(cbind(acf(bayRegProp1start1$chainNoBurnIn[,2], plot = FALSE)$lag,
                          acf(bayRegProp1start1$chainNoBurnIn[,2], plot = FALSE)$acf))
colnames(ac2) = c("lag", "acf")

ac3 = as.data.frame(cbind(acf(bayRegProp1start1$chainNoBurnIn[,3], plot = FALSE)$lag,
                          acf(bayRegProp1start1$chainNoBurnIn[,3], plot = FALSE)$acf))
colnames(ac3) = c("lag", "acf")

acfPlot1 = ggplot(data = ac1, mapping = aes(x = lag, y = acf)) +
       geom_hline(aes(yintercept = 0)) +
       geom_segment(mapping = aes(xend = lag, yend = 0)) + ggtitle("ACF beta0")

acfPlot2 = ggplot(data = ac2, mapping = aes(x = lag, y = acf)) +
       geom_hline(aes(yintercept = 0)) +
       geom_segment(mapping = aes(xend = lag, yend = 0)) + ggtitle("ACF beta1")

acfPlot3 = ggplot(data = ac3, mapping = aes(x = lag, y = acf)) +
       geom_hline(aes(yintercept = 0)) +
       geom_segment(mapping = aes(xend = lag, yend = 0))+ ggtitle("ACF SD")

multiplot(acfPlot1, acfPlot2, acfPlot3)
```

Für alle drei Parameter ist eine starke Autokorrelationzu verzeichnen, wünschenswert wäre allerdings eine möglichst geringe Autokorrelation. Um dies zu erreichen, könnten wir beispielsweise mit Rejection-Sampling arbeiten und nur jede k-te, nicht-verworfene Iteration akzeptieren, worauf wir an dieser Stelle allerdings verzichten.

### Dichte-Plots der Parameter
Zuletzt plotten wir die Kerndichtechätzungen der drei Parameter. Dies ermöglicht uns einen besseren Einblick in die Verteilung der einzelnen Parameter.

```{r kde}
kd1 = ggplot(bayRegProp1start1$chainNoBurnIn, aes(beta0)) + geom_density(color = 1, fill = 1, alpha = 0.1) + geom_vline(xintercept = beta0, color = 4) + ggtitle("KDE beta0") + geom_vline(xintercept = mean(bayRegProp1start1$chainNoBurnIn[,1]), color = 6)

kd2 = ggplot(bayRegProp1start1$chainNoBurnIn, aes(beta1)) + geom_density(color = 2, fill = 2, alpha = 0.1) + geom_vline(xintercept = beta1, color = 4) + ggtitle("KDE beta1") + ggtitle("KDE beta0") + geom_vline(xintercept = mean(bayRegProp1start1$chainNoBurnIn[,2]), color = 6)

kd3 = ggplot(bayRegProp1start1$chainNoBurnIn, aes(SD)) + geom_density(color = 3, fill = 3, alpha = 0.1) + geom_vline(xintercept = standardDeviation, color = 4) + ggtitle("KDE SD") + ggtitle("KDE beta0") + geom_vline(xintercept = mean(bayRegProp1start1$chainNoBurnIn[,3]), color = 6)

multiplot(kd1, kd2, kd3)


```

Offensichtlich überschätzt der Bayes-Schätzer (Mittelwert der Markov-Kette, violette Linie) für beta0 den wahren Parameter (blaue Linie) und unterschätzt den wahren Parameter für beta1. Im Fall der Varianz trifft der Bayes-Schätzer den wahren Wert sehr gut. Die Abweichungen könnten im vermutlich durch höhere Varianz in den Prior-Verteilungen der Beta-Parameter reduziert werden, was allderdings nicht mehr dem eigentlichen Bayes-Paradigma der Einbindung von a-priori Information entsprechen würde. Nichtsdestotrotz befinden sich die wahren Parameter in einem angemessenen Dichtebereich der Posterior Randdichteverteilungen.

#Schlussbemerkung
Im Verlauf unserer Arbeit haben wir eine R-Funktion zur Linearen Bayes-Regression mithilfe des Metropolis-Algorithmus erstellt, die wir dann nach Ermittlung geeigneter Parameter für die Normal-Inversegamma Priors auf einen synthetisch erstellten Datensatz angewandt haben. Die Acceptance-Rate ist dabei als stark abhängig von der Wahl passender Proposal-Densities, bzw. geeigneter Skalenparameter ebendieser aufgefallen. Hinsichtlich des Konvergenzverhaltens haben wir eine rasche Konvergenz festgestellt.  
Die Parameterschätzung selbst erzielte gute Ergebnisse, die besonders für die Standardabweichung nahe am wahren Wert lagen.  
  
Es hat sich gezeigt, dass Bayesianische Methoden eine potente Erweiterung der Methoden der klassischen Statistik darstellen, die gerade in Zeiten von Big Data und Machine Learning beispielsweise in Form von Gaussian Processes einen hohen Stellenwert besitzen und in Zukunft wohl auch weiterhin besitzen werden.